{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Social-Feedback-Enhanced-Argument-Claim-Extraction-from-News-Headlines.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Goqi6DaFplZl",
        "NEcpv1nfqmeO",
        "MZcZp45d7Neq",
        "QPukiJqJY5oC"
      ],
      "toc_visible": true,
      "mount_file_id": "1bS71swGWkyhRfmMmHWx-bNdo2JCvUKOg",
      "authorship_tag": "ABX9TyOKQEro9NWr+z4P1y+Ya7Px",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WeiChuen99/Social-Feedback-Enhanced-Argument-Claim-Extraction-from-News-Headlines/blob/main/Social_Feedback_Enhanced_Argument_Claim_Extraction_from_News_Headlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goqi6DaFplZl"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncvXx9xVu7yS"
      },
      "source": [
        "%%capture\n",
        "# !pip3 install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDBZxLVF-4ES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13cc9dd-7616-474b-aa97-642f19733366"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 300)\n",
        "# import praw #reddit data api\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sn\n",
        "import re #regex\n",
        "\n",
        "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer #VADER sentiment model\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.test.gpu_device_name() #run to make sure tensorflow is connected to gpu (if applicable)\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.translate.bleu_score import sentence_bleu \n",
        "#import flair\n",
        "\n",
        "from textblob import TextBlob\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB_aZ7fSyxsy"
      },
      "source": [
        "%%capture\n",
        "pip install stop-words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPq8U8UZzTQI"
      },
      "source": [
        "%%capture\n",
        " !pip install pyLDAvis==3.2.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYsaJT6dcK4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be395e9-51c5-4c9d-8a1e-d87a8d129b4f"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from stop_words import get_stop_words\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import pyLDAvis.gensim\n",
        "from nltk.corpus import stopwords \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import pprint\n",
        "import warnings\n",
        "\n",
        "from pandas import read_excel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vH8lS-eqdW_"
      },
      "source": [
        "#Reddit Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTF2dFA4cVYk"
      },
      "source": [
        "#reddit submissions scrapping\n",
        "\n",
        "def get_pushshift_data(query, after, before, sub):\n",
        "  # url = 'https://api.pushshift.io/reddit/search/submission/?title=' + str(query) + '&size=500&after=' + str(\n",
        "  #     after) + '&before=' + str(before) + '&subreddit=' + str(sub)\n",
        "  url = 'https://api.pushshift.io/reddit/search/submission/?after=' + str(\n",
        "      after) + '&before=' + str(before) + '&subreddit=' + str(sub)\n",
        "  print(url)\n",
        "  r = requests.get(url)\n",
        "  try:\n",
        "    data = json.loads(r.text)\n",
        "  except:\n",
        "    print(\"server busy\")\n",
        "    return []\n",
        "  return data['data']\n",
        "\n",
        "\n",
        "def collect_sub_data(subm):\n",
        "  sub_data = list()  # list to store data points\n",
        "  headline = subm['title']\n",
        "  sub_id = subm['id']\n",
        "  created = datetime.datetime.fromtimestamp(subm['created_utc'])\n",
        "  numComms = subm['num_comments']\n",
        "\n",
        "  #only use submissions with sufficient comments\n",
        "  if numComms > 100:\n",
        "    sub_data.append((sub_id, headline, created, numComms))\n",
        "    sub_stats[sub_id] = sub_data\n",
        "\n",
        "\n",
        "def write_subs_to_file(filename):\n",
        "  upload_count = 0\n",
        "  if os.path.exists(filename):\n",
        "      keep_header = False\n",
        "  else:\n",
        "      keep_header = True\n",
        "\n",
        "  with open(filename, 'a', newline='') as file:\n",
        "      a = csv.writer(file, delimiter=',')\n",
        "      headers = ['sub_id', 'headline', 'publish_date', 'num_of_comments']\n",
        "      if keep_header:\n",
        "          a.writerow(headers)\n",
        "\n",
        "      for sub in all_subs:\n",
        "          a.writerow(all_subs[sub][0])\n",
        "          upload_count += 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # Download reddit posts from sub_reddit with keywords given by key_word\n",
        "\n",
        "  sub_reddit = 'science'\n",
        "  key_word = 'covid'\n",
        "\n",
        "  output_filename = 'reddit_data.csv'\n",
        "  # search all the posts from start_date to end_date overall\n",
        "  start_date = datetime.datetime(2020, 11, 11, 0)\n",
        "  end_date = datetime.datetime(2020, 11, 15, 0)\n",
        "\n",
        "  # in each itration get reddit posts for one day, to avoid getting blocked by server\n",
        "  one_day = datetime.timedelta(hours=24)\n",
        "  after_date = start_date\n",
        "  after = str(int(after_date.timestamp()))\n",
        "  before_date = start_date + one_day\n",
        "  before = str(int(before_date.timestamp()))\n",
        "\n",
        "  all_subs = {}\n",
        "  while after_date < end_date:\n",
        "    print('-' * 80)\n",
        "    print(after_date, ' -> ', before_date)\n",
        "    print('-' * 80)\n",
        "\n",
        "    sub_count = 0\n",
        "    sub_stats = {}\n",
        "    sub_ids = []\n",
        "\n",
        "    data = get_pushshift_data(key_word, after, before, sub_reddit)\n",
        "\n",
        "    max_count = 100\n",
        "    count = 0\n",
        "    while len(data) > 0 and count < max_count:\n",
        "        # print('count ', count)\n",
        "        for submission in data:\n",
        "            collect_sub_data(submission)\n",
        "            sub_count += 1\n",
        "\n",
        "        # print(len(data))\n",
        "        print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
        "        after = data[-1]['created_utc']\n",
        "        data = get_pushshift_data(key_word, after, before, sub_reddit)\n",
        "        all_subs.update(sub_stats)\n",
        "        # print(data)\n",
        "        # print(data['data'][0]['author'])\n",
        "        count = count + 1\n",
        "\n",
        "    # move to next day\n",
        "    after_date += one_day\n",
        "    after = str(int(after_date.timestamp()))\n",
        "    before_date += one_day\n",
        "    before = str(int(before_date.timestamp()))\n",
        "\n",
        "    # randomly sleep before starting next iteration\n",
        "    time.sleep(np.random.randint(1, 3))\n",
        "\n",
        "  # keep saving data collected in each iteration\n",
        "  write_subs_to_file(output_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_ALKTDVnQIZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "3654601f-3ad3-4848-cb52-a1e076eba556"
      },
      "source": [
        "sheet_name = 'reddit_data' # change it to your sheet name, you can find your sheet name at the bottom left of your excel file\n",
        "file_name = 'reddit_data_labeled.xlsx' # change it to the name of your excel file\n",
        "df_labeled = read_excel(file_name, sheet_name = sheet_name)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2aa167c04c20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msheet_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'reddit_data'\u001b[0m \u001b[0;31m# change it to your sheet name, you can find your sheet name at the bottom left of your excel file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'reddit_data_labeled.xlsx'\u001b[0m \u001b[0;31m# change it to the name of your excel file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_labeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'read_excel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgqCoCUV4RSv"
      },
      "source": [
        "#read csv into dataframe\n",
        "# df_subs = pd.read_csv('reddit_data.csv', usecols= ['post_id', 'headline', 'publish_date', 'num_of_comments'])\n",
        "df_subs = df_labeled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5MppfSrQ_Ti"
      },
      "source": [
        "#reddit comments scrapping\n",
        "\n",
        "def get_pushshift_com_data(post_id, limit):\n",
        "    #https://api.pushshift.io/reddit/comment/search/?link_id=j32xmk&limit=2&sort=asc\n",
        "    # url = 'https://api.pushshift.io/reddit/comment/search/?link_id=' + str(post_id) + '&limit=' + str(\n",
        "    #     limit) + '&sort=asc'\n",
        "    url = 'https://api.pushshift.io/reddit/comment/search/?link_id=' + str(post_id) + '&limit=' + str(\n",
        "        limit)+ '&sort=asc'\n",
        "    print(url)\n",
        "    r = requests.get(url)\n",
        "    try:\n",
        "      url_data = json.loads(r.text)\n",
        "    except:\n",
        "      print(\"server error\")\n",
        "      return []\n",
        "    return url_data['data']\n",
        "\n",
        "\n",
        "def collect_com_data(subm):\n",
        "  com_id = subm['id']\n",
        "  parent_sub_id = subm['link_id']  \n",
        "  parent_sub_id = parent_sub_id[3:] #id of submission\n",
        "  parent_id = subm['parent_id']     #to determine level of comment\n",
        "  body = subm['body']               #comment body\n",
        "  score = subm['score']             #upvotes or downvotes\n",
        "\n",
        "  return com_id, parent_sub_id, parent_id, body, score\n",
        "\n",
        "def write_coms_to_file(filename):\n",
        "    upload_count = 0\n",
        "    if os.path.exists(filename):\n",
        "        keep_header = False\n",
        "    else:\n",
        "        keep_header = True\n",
        "\n",
        "    with open(filename, 'a', newline='') as file:\n",
        "        a = csv.writer(file, delimiter=',')\n",
        "        headers = ['sub_id', 'comment']\n",
        "        if keep_header:\n",
        "            a.writerow(headers)\n",
        "        for com in com_stats:\n",
        "            a.writerow(com_stats[com][0])\n",
        "            upload_count += 1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  com_stats = {}\n",
        "  com_output_filename = \"reddit_com_data.csv\"\n",
        "  limit_num_comments = 500\n",
        "  ####### will increase later if using first level comments only \n",
        "\n",
        "  for sub_id in df_subs[\"sub_id\"]:\n",
        "    data = []  # list to store data points\n",
        "    merged_body = \"\"\n",
        "    num_comments = 0\n",
        "    com_data = get_pushshift_com_data(sub_id, limit_num_comments)\n",
        "    for comment in com_data:\n",
        "      com_id, parent_sub_id, parent_id, body, score = collect_com_data(comment)\n",
        "      if body != \"[deleted]\" and body != \"[removed]\" and body != \"\" and parent_id[0:2] == \"t3\" and len(body) > 6:\n",
        "        merged_body = merged_body + body\n",
        "        num_comments = num_comments + 1\n",
        "        if num_comments == 10:\n",
        "          break\n",
        "        \n",
        "    data.append((parent_sub_id, merged_body))\n",
        "    com_stats[sub_id] = data\n",
        "\n",
        "    #randomly sleep before starting next iteration\n",
        "    time.sleep(np.random.randint(1, 3))\n",
        "\n",
        "  # keep saving data collected in each iteration\n",
        "  write_coms_to_file(com_output_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5erramGA6iz"
      },
      "source": [
        "#read csv into dataframe\n",
        "df_coms = pd.read_csv('reddit_com_data.csv', usecols= ['sub_id', 'comment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyBXLmDFY-1c"
      },
      "source": [
        "import json\n",
        "\n",
        "# Function to convert a CSV to JSON\n",
        "# Takes the file paths as arguments\n",
        "def make_json(csvFilePath, jsonFilePath):\n",
        "    # Open a csv reader called DictReader\n",
        "    with open(csvFilePath, encoding='utf-8') as csvf:\n",
        "      csvReader = csv.DictReader(csvf)\n",
        "\n",
        "      # Convert each row into a dictionary \n",
        "      # and add it to data\n",
        "      for rows in csvReader:\n",
        "      # Open a json writer, and use the json.dumps() \n",
        "      # function to dump data\n",
        "        with open(jsonFilePath, 'a', encoding='utf-8') as jsonf:\n",
        "            jsonf.write(json.dumps(rows))\n",
        "            jsonf.write(\"\\n\")\n",
        "         \n",
        "# Driver Code\n",
        " \n",
        "# Decide the two file paths according to your \n",
        "# computer system\n",
        "subcsvFilePath = r'reddit_data.csv'\n",
        "subjsonFilePath = r'reddit_data.json'\n",
        "\n",
        "comcsvFilePath = r'reddit_com_data.csv'\n",
        "comjsonFilePath = r'reddit_com_data.json'\n",
        "\n",
        "# Call the make_json function\n",
        "# make_json(subcsvFilePath, subjsonFilePath)\n",
        "# make_json(comcsvFilePath, comjsonFilePath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cuVqAUPnj90"
      },
      "source": [
        "#Topic Modeling using LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA6hGd2byf3P"
      },
      "source": [
        "#read csv into dataframe\n",
        "df_labeled = pd.read_csv('reddit_data_evaluated.csv', usecols= ['headlines', 'comment', 'labeled_argument'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mAgB4VvzY-p"
      },
      "source": [
        "df_labeled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roMpUivxY23M"
      },
      "source": [
        "#ignore depreciated warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def preprocess_data (document):\n",
        "  #Initiating Tokenizer, Lemmatizer and Stop Words\n",
        "  pattern = r'\\b[^\\d\\W]+\\b'\n",
        "  tokenizer = RegexpTokenizer(pattern)\n",
        "  en_stop = get_stop_words('en')\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  remove_words = list(stopwords.words('english'))\n",
        "\n",
        "  #Perform Tokenization, Words removal, and Lemmatization\n",
        "\n",
        "  # clean and tokenize document string\n",
        "  raw = str(document).lower()\n",
        "  tokens = tokenizer.tokenize(raw)\n",
        "\n",
        "  # remove stop words from tokens\n",
        "  stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n",
        "  \n",
        "  # remove stop words from tokens\n",
        "  stopped_tokens_new = [raw for raw in stopped_tokens if not raw in remove_words]\n",
        "  \n",
        "  # lemmatize tokens\n",
        "  lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens_new]\n",
        "  \n",
        "  # remove word containing only single char\n",
        "  new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n",
        "  \n",
        "  # add tokens to list\n",
        "  texts = []\n",
        "  texts.append(new_lemma_tokens)\n",
        "\n",
        "  # turn our tokenized documents into a id <-> term dictionary\n",
        "  dictionary = corpora.Dictionary(texts)\n",
        "  # convert tokenized documents into a document-term matrix\n",
        "  corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "  \n",
        "  return dictionary, corpus\n",
        "\n",
        "def perform_lda (document):\n",
        "  # list for tokenized documents in loop\n",
        "  lda_topics = []\n",
        "  lda_models = []\n",
        "  list_corpus = []\n",
        "  list_dictionary = []\n",
        "\n",
        "  for row in document:\n",
        "    #preprocess data\n",
        "    dictionary, corpus = preprocess_data(row)\n",
        "\n",
        "    #Generate LDA model\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=1, id2word = dictionary, passes=20)\n",
        "    topics = ldamodel.top_topics(corpus,topn=7)\n",
        "\n",
        "    #convert topic array into string\n",
        "    topics = topics[0][0]\n",
        "    topics_str = \"\"\n",
        "    for topic_word in topics:\n",
        "      topics_str = topics_str + \" \" + topic_word[1]\n",
        "\n",
        "\n",
        "    lda_models.append(ldamodel)\n",
        "    lda_topics.append(topics_str)\n",
        "    list_corpus.append(corpus)\n",
        "    list_dictionary.append(dictionary)\n",
        "\n",
        "  return lda_topics, lda_models, list_corpus, list_dictionary\n",
        "\n",
        "\n",
        "baseline_lda, baseline_models, baseline_corpus, baseline_dictionary = perform_lda(df_labeled[\"headlines\"])\n",
        "df_labeled[\"headline_lda\"] = baseline_lda\n",
        "\n",
        "df_labeled[\"headline_and_comment\"] = df_labeled[\"headlines\"] + df_labeled[\"comment\"]\n",
        "proposed_lda, proposed_models, proposed_corpus, proposed_dictionary = perform_lda(df_labeled[\"headline_and_comment\"])\n",
        "df_labeled[\"social_aspect_lda\"] = proposed_lda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWsYp2SeWr82"
      },
      "source": [
        "#create new dataframe with relevent data\n",
        "dfnew = pd.concat([df_labeled[\"headlines\"], df_labeled[\"labeled_argument\"],\n",
        "                   df_labeled[\"headline_lda\"], df_labeled[\"social_aspect_lda\"]], axis = 1)\n",
        "dfnew.columns = [\"headlines\", \"labeled_argument\", \"headline_lda\", \"social_aspect_lda\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnCRXHW4OfmV"
      },
      "source": [
        "def evaluate(dataframe, list_string):\n",
        "  list_score = []\n",
        "  for index, row in dataframe.iterrows():\n",
        "    reference = row[list_string[0]]\n",
        "    candidate = row[list_string[1]]\n",
        "    score = sentence_bleu([reference.split(\" \")], candidate.split(\" \"))\n",
        "    list_score.append(score)\n",
        "  print(\"average bleu score = \", sum(list_score)/len(list_score))\n",
        "  return list_score\n",
        "\n",
        "#evaluate baseline method\n",
        "baseline_bleu_score = evaluate(dfnew, [\"labeled_argument\", \"headline_lda\"])\n",
        "\n",
        "#evaluate social aspect method\n",
        "social_aspect_bleu_score = evaluate(dfnew, [\"labeled_argument\", \"social_aspect_lda\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sms3SFBZ-kZZ"
      },
      "source": [
        "dfnew[\"baseline_bleu_score\"] = baseline_bleu_score\n",
        "dfnew[\"social_aspect_bleu_score\"] = social_aspect_bleu_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOLuaUscikTQ"
      },
      "source": [
        "dfnew.to_excel(r'reddit_data_evaluated.xlsx', sheet_name='reddit_data', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpocozjXAc8p"
      },
      "source": [
        "#Visualise LDA model\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_models[0], list_corpus[0], list_dictionary[0], mds='mmds')\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEcpv1nfqmeO"
      },
      "source": [
        "#Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i9NfHxDvQqv"
      },
      "source": [
        "#sentiment analysis\n",
        "\n",
        "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer #VADER sentiment model\n",
        "\n",
        "#using flair pre-trained model\n",
        "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n",
        "fmt = '%Y-%m-%d %H:00:00'\n",
        "#vader sentiment model\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "def get_sentiment_val_for_flair(sentiments):\n",
        "    \"\"\"\n",
        "    parse input of the format [NEGATIVE (0.9284018874168396)] and return +ve or -ve float value\n",
        "    :param sentiments:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    total_sentiment = str(sentiments)\n",
        "    neg = 'NEGATIVE' in total_sentiment\n",
        "    if neg:\n",
        "        total_sentiment = total_sentiment.replace('NEGATIVE', '')\n",
        "    else:\n",
        "        total_sentiment = total_sentiment.replace('POSITIVE', '')\n",
        "\n",
        "    total_sentiment = total_sentiment.replace('(', '').replace('[', '').replace(')', '').replace(']', '')\n",
        "\n",
        "    val = float(total_sentiment)\n",
        "    if neg:\n",
        "        return -val\n",
        "    return val\n",
        "\n",
        "\n",
        "def get_sentiment_report(input_filename, output_filename):\n",
        "  #read csv input\n",
        "    df = pd.read_csv(input_filename)\n",
        "    df = df[['title', 'selftext', 'publish_date']]\n",
        "    df = df.fillna('')\n",
        "\n",
        "    df['text'] = df['title'] + ' ' + df['selftext']\n",
        "    df.set_index('publish_date', inplace=True)\n",
        "    df.drop(['title', 'selftext'], axis=1, inplace=True)\n",
        "\n",
        "    for row_i, row in df.iterrows():\n",
        "        tb_sentiment_polarity_dict = dict()\n",
        "        tb_sentiment_subjectivity_dict = dict()\n",
        "        flair_sentiment_dict = dict()\n",
        "\n",
        "        sid_pos_dict = dict()\n",
        "        sid_neg_dict = dict()\n",
        "        sid_neu_dict = dict()\n",
        "        sid_com_dict = dict()\n",
        "\n",
        "        data = row['text']\n",
        "        print(row_i)\n",
        "        print(data[0:15])\n",
        "        flair_s = flair.data.Sentence(data)\n",
        "        flair_sentiment.predict(flair_s)\n",
        "        flair_total_sentiment = flair_s.labels\n",
        "        flair_val = get_sentiment_val_for_flair(flair_total_sentiment)\n",
        "\n",
        "        flair_sentiment_dict[str(row_i)] = flair_val\n",
        "        tb_sentiment_polarity_dict[str(row_i)] = TextBlob(data).sentiment[0]\n",
        "        tb_sentiment_subjectivity_dict[str(row_i)] = TextBlob(data).sentiment[1]\n",
        "\n",
        "        ss = sid.polarity_scores(data)\n",
        "        sid_pos_dict[str(row_i)] = ss['pos']\n",
        "        sid_neg_dict[str(row_i)] = ss['neg']\n",
        "        sid_neu_dict[str(row_i)] = ss['neu']\n",
        "        sid_com_dict[str(row_i)] = ss['compound']\n",
        "\n",
        "        flair_df = pd.DataFrame.from_dict(flair_sentiment_dict, orient='index', columns=['reddit_flair'])\n",
        "        flair_df.index.name = 'timestamp'\n",
        "\n",
        "        tb_polarity_df = pd.DataFrame.from_dict(tb_sentiment_polarity_dict, orient='index',\n",
        "                                                columns=['reddit_tb_polarity'])\n",
        "        tb_polarity_df.index.name = 'timestamp'\n",
        "\n",
        "        tb_subjectivity_df = pd.DataFrame.from_dict(tb_sentiment_subjectivity_dict, orient='index',\n",
        "                                                    columns=['reddit_tb_subjectivity'])\n",
        "        tb_subjectivity_df.index.name = 'timestamp'\n",
        "\n",
        "        sid_pos_df = pd.DataFrame.from_dict(sid_pos_dict, orient='index',\n",
        "                                            columns=['reddit_sid_pos'])\n",
        "        sid_pos_df.index.name = 'timestamp'\n",
        "\n",
        "        sid_neg_df = pd.DataFrame.from_dict(sid_neg_dict, orient='index',\n",
        "                                            columns=['reddit_sid_neg'])\n",
        "        sid_neg_df.index.name = 'timestamp'\n",
        "\n",
        "        sid_neu_df = pd.DataFrame.from_dict(sid_neu_dict, orient='index',\n",
        "                                            columns=['reddit_sid_neu'])\n",
        "        sid_neu_df.index.name = 'timestamp'\n",
        "\n",
        "        sid_com_df = pd.DataFrame.from_dict(sid_com_dict, orient='index',\n",
        "                                            columns=['reddit_sid_com'])\n",
        "        sid_com_df.index.name = 'timestamp'\n",
        "\n",
        "        final_senti_df = pd.concat([flair_df, tb_polarity_df, tb_subjectivity_df, sid_pos_df, sid_neg_df, sid_neu_df,\n",
        "        \t\t\t\t\t\t\tsid_com_df], axis=1)\n",
        "\n",
        "        if os.path.exists(output_filename):\n",
        "            keep_header = False\n",
        "        else:\n",
        "            keep_header = True\n",
        "\n",
        "        final_senti_df.to_csv(output_filename, mode='a', header=keep_header)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def clean_sentiment_report(input_filename, output_filename):\n",
        "    # drop duplicates and sort\n",
        "    master_df = pd.read_csv(input_filename, index_col=0)\n",
        "    master_df.index = pd.to_datetime(master_df.index)\n",
        "    idx = np.unique(master_df.index, return_index=True)[1]\n",
        "    master_df = master_df.iloc[idx]\n",
        "    master_df.to_csv(output_filename)\n",
        "\n",
        "\n",
        "def bucketize_sentiment_report(input_filename, output_filename):\n",
        "    start_date_time_obj = datetime.datetime(2020, 10, 1, 0)\n",
        "    end_date_time_obj = datetime.datetime(2020, 10, 31, 0)\n",
        "    hr1 = datetime.timedelta(hours=1)\n",
        "    curr_date_time_obj = start_date_time_obj\n",
        "    in_df = pd.read_csv(input_filename)\n",
        "\n",
        "\n",
        "    out_dict = dict()\n",
        "\n",
        "    while curr_date_time_obj <= end_date_time_obj:\n",
        "        curr_timestamp = curr_date_time_obj.strftime(format=fmt)\n",
        "        # print(curr_timestamp)\n",
        "        # create data dict with all possible timestamps and dummy value of reddit_flair\n",
        "        # reddit_flair is chosen just randomly as a placeholder\n",
        "        out_dict[curr_timestamp] = 0\n",
        "        curr_date_time_obj += hr1\n",
        "\n",
        "    out_df = pd.DataFrame.from_dict(out_dict, orient='index',\n",
        "                                    columns=['reddit_flair'])\n",
        "\n",
        "    # print(out_dict)\n",
        "    out_df.index.name = 'timestamp'\n",
        "    # populate more colums\n",
        "    out_df['reddit_flair_count'] = 0\n",
        "    out_df['reddit_tb_polarity'] = 0\n",
        "    out_df['reddit_tb_polarity_count'] = 0\n",
        "    out_df['reddit_tb_subjectivity'] = 0\n",
        "    out_df['reddit_tb_subjectivity_count'] = 0\n",
        "    out_df['reddit_sid_neg'] = 0\n",
        "    out_df['reddit_sid_neu'] = 0\n",
        "    out_df['reddit_sid_com'] = 0\n",
        "    out_df['reddit_sid_count'] = 0\n",
        "\n",
        "    for i in range(len(in_df)):\n",
        "        timestamp = in_df.loc[i, 'timestamp']\n",
        "        out_key = datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
        "        # timestamp is current plus few minutes or seconds, so collect all these data in the bucket of next hour\n",
        "        out_key += hr1\n",
        "        out_key = out_key.strftime(format='%Y-%m-%d %H:00:00')\n",
        "        #print(out_key)\n",
        "        # add up all values and count how many values we have added. In next pass we would normalize the values\n",
        "        try:\n",
        "            out_df.loc[out_key, 'reddit_flair'] += in_df.loc[i, 'reddit_flair']\n",
        "            out_df.loc[out_key, 'reddit_flair_count'] += 1\n",
        "            out_df.loc[out_key, 'reddit_tb_polarity'] += in_df.loc[i, 'reddit_tb_polarity']\n",
        "            out_df.loc[out_key, 'reddit_tb_polarity_count'] += 1\n",
        "            out_df.loc[out_key, 'reddit_tb_subjectivity'] += in_df.loc[i, 'reddit_tb_subjectivity']\n",
        "            out_df.loc[out_key, 'reddit_tb_subjectivity_count'] += 1\n",
        "            out_df.loc[out_key, 'reddit_sid_pos'] += in_df.loc[i, 'reddit_sid_pos']\n",
        "            out_df.loc[out_key, 'reddit_sid_neg'] += in_df.loc[i, 'reddit_sid_neg']\n",
        "            out_df.loc[out_key, 'reddit_sid_neu'] += in_df.loc[i, 'reddit_sid_neu']\n",
        "            out_df.loc[out_key, 'reddit_sid_com'] += in_df.loc[i, 'reddit_sid_com']\n",
        "            out_df.loc[out_key, 'reddit_sid_count'] += 1\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # make timestamp as a column and reindex the dataframe to make loc method happy\n",
        "    out_df['timestamp'] = out_df.index\n",
        "    out_df.index = range(len(out_df))\n",
        "\n",
        "    for i in range(len(out_df)):\n",
        "        #print(out_df.loc[i, 'timestamp'])\n",
        "        # normalize the values\n",
        "        if out_df.loc[i, 'reddit_flair_count'] == 0:\n",
        "            out_df.loc[i, 'reddit_flair'] = 0\n",
        "        else:\n",
        "            out_df.loc[i, 'reddit_flair'] /= out_df.loc[i, 'reddit_flair_count']\n",
        "\n",
        "        if out_df.loc[i, 'reddit_tb_polarity_count'] == 0:\n",
        "            out_df.loc[i, 'reddit_tb_polarity'] = 0\n",
        "        else:\n",
        "            out_df.loc[i, 'reddit_tb_polarity'] /= out_df.loc[i, 'reddit_tb_polarity_count']\n",
        "\n",
        "        if out_df.loc[i, 'reddit_tb_subjectivity_count'] == 0:\n",
        "            out_df.loc[i, 'reddit_tb_subjectivity'] = 0\n",
        "        else:\n",
        "            out_df.loc[i, 'reddit_tb_subjectivity'] /= out_df.loc[i, 'reddit_tb_subjectivity_count']\n",
        "\n",
        "        if out_df.loc[i, 'reddit_sid_count'] == 0:\n",
        "            out_df.loc[i, 'reddit_sid_pos'] = 0\n",
        "            out_df.loc[i, 'reddit_sid_neg'] = 0\n",
        "            out_df.loc[i, 'reddit_sid_neu'] = 0\n",
        "            out_df.loc[i, 'reddit_sid_com'] = 0\n",
        "        else:\n",
        "            out_df.loc[i, 'reddit_sid_pos'] /= out_df.loc[i, 'reddit_sid_count']\n",
        "            out_df.loc[i, 'reddit_sid_neg'] /= out_df.loc[i, 'reddit_sid_count']\n",
        "            out_df.loc[i, 'reddit_sid_neu'] /= out_df.loc[i, 'reddit_sid_count']\n",
        "            out_df.loc[i, 'reddit_sid_com'] /= out_df.loc[i, 'reddit_sid_count']\n",
        "\n",
        "        if os.path.exists(output_filename):\n",
        "            keep_header = False\n",
        "        else:\n",
        "            keep_header = True\n",
        "\n",
        "    out_df.drop(['reddit_flair_count', 'reddit_tb_polarity_count', 'reddit_tb_subjectivity_count','reddit_sid_count'], axis=1,\n",
        "                inplace=True)\n",
        "    # change back index to timestamp to save the data in csv\n",
        "    out_df.set_index('timestamp', inplace=True)\n",
        "    out_df.to_csv(output_filename)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_filename = 'reddit_data.csv'\n",
        "    output_sentiment_filename = input_filename[0:-4] + '_sentiment.csv'\n",
        "\n",
        "    # read input_filename (which can be generated by download_data_from_reddit.py script) and performs\n",
        "    # sentiment analyis of the text data\n",
        "    get_sentiment_report(input_filename, output_sentiment_filename)\n",
        "    output_sentiment_bucketize_filename = output_sentiment_filename[0:-4] + '_bucketized.csv'\n",
        "\n",
        "    # reddit posts can land anytime. Collect all the posts (and its sentiment reports) landed on a given hour (0 to 59 minutes)\n",
        "    # and bucketize them all into the corresponding hour\n",
        "    bucketize_sentiment_report(output_sentiment_filename, output_sentiment_bucketize_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZcZp45d7Neq"
      },
      "source": [
        "# Aspect Based Argument Mining (testing)(dataset only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTt9uw_Y7Lde"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#################################################################################\n",
        "# SENTENCES\n",
        "\n",
        "ABAM_DATA_SENTENCES_without_sentences = pd.read_csv(\n",
        "        '/content/drive/MyDrive/Colab Notebooks/FYP/ABAM_DATA_SENTENCES.tsv', \n",
        "        sep='\\t')\n",
        "print(len(ABAM_DATA_SENTENCES_without_sentences))\n",
        "\n",
        "ABAM_SENTENCES = pd.read_csv(\n",
        "        '/content/drive/MyDrive/Colab Notebooks/FYP/ABAM_SENTENCES.tsv', \n",
        "        sep='\\t')\n",
        "print(len(ABAM_SENTENCES))\n",
        "\n",
        "assert len(ABAM_DATA_SENTENCES_without_sentences)==len(ABAM_SENTENCES)\n",
        "\n",
        "ABAM_DATA_SENTENCES = pd.merge(\n",
        "        ABAM_DATA_SENTENCES_without_sentences[['topic', 'sentence_hash', 'stance', 'aspect', 'inner', 'cross']], \n",
        "        ABAM_SENTENCES, \n",
        "        on='sentence_hash')\n",
        "ABAM_DATA_SENTENCES = ABAM_DATA_SENTENCES[['topic', 'sentence_hash', 'sentence', 'stance', 'aspect', 'inner', 'cross']]\n",
        "print(len(ABAM_DATA_SENTENCES))\n",
        "\n",
        "print(ABAM_DATA_SENTENCES[['topic', 'sentence_hash']].groupby('topic').count())\n",
        "\n",
        "#################################################################################\n",
        "# SEGMENTS\n",
        "\n",
        "ABAM_DATA_SEGMENTS_without_segments = pd.read_csv(\n",
        "        '/content/drive/MyDrive/Colab Notebooks/FYP/ABAM_DATA_SEGMENTS.tsv', \n",
        "        sep='\\t')\n",
        "print(len(ABAM_DATA_SEGMENTS_without_segments))\n",
        "\n",
        "ABAM_SEGMENTS = pd.read_csv(\n",
        "        '/content/drive/MyDrive/Colab Notebooks/FYP/ABAM_SEGMENTS.tsv', \n",
        "        sep='\\t')\n",
        "\n",
        "\n",
        "print(len(ABAM_SEGMENTS))\n",
        "\n",
        "assert len(ABAM_DATA_SEGMENTS_without_segments)==len(ABAM_SEGMENTS)\n",
        "\n",
        "ABAM_DATA_SEGMENTS = pd.merge(\n",
        "        ABAM_DATA_SEGMENTS_without_segments[['topic', 'sentence_hash', 'segment_count', 'segment_hash', 'stance', 'aspect', 'inner', 'cross']], \n",
        "        ABAM_SEGMENTS, on='segment_hash')\n",
        "ABAM_DATA_SEGMENTS = ABAM_DATA_SEGMENTS[['topic', 'sentence_hash', 'segment_count', 'segment_hash', 'segment', 'stance', 'aspect', 'inner', 'cross']]\n",
        "print(len(ABAM_DATA_SEGMENTS))\n",
        "\n",
        "print(ABAM_DATA_SEGMENTS[['topic', 'segment_hash']].groupby('topic').count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPukiJqJY5oC"
      },
      "source": [
        "#Topic Modelling Ori code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3Q9A0-JnuRq"
      },
      "source": [
        "#Initiating Tokenizer, Lemmatizer and Stop Words\n",
        "pattern = r'\\b[^\\d\\W]+\\b'\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "en_stop = get_stop_words('en')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "remove_words = list(stopwords.words('english'))\n",
        "\n",
        "#Perform Tokenization, Words removal, and Lemmatization\n",
        "df[\"Description\"] = df[\"headline\"]+\". \" +df[\"short_description\"]\n",
        "# list for tokenized documents in loop\n",
        "texts = []\n",
        "\n",
        "# loop through document list\n",
        "for i in df['Description'].iteritems():\n",
        "    # clean and tokenize document string\n",
        "    raw = str(i[1]).lower()\n",
        "    tokens = tokenizer.tokenize(raw)\n",
        "\n",
        "    # remove stop words from tokens\n",
        "    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n",
        "    \n",
        "    # remove stop words from tokens\n",
        "    stopped_tokens_new = [raw for raw in stopped_tokens if not raw in remove_words]\n",
        "    \n",
        "    # lemmatize tokens\n",
        "    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens_new]\n",
        "    \n",
        "    # remove word containing only single char\n",
        "    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n",
        "    \n",
        "    # add tokens to list\n",
        "    texts.append(new_lemma_tokens)\n",
        "\n",
        "# sample data\n",
        "print(texts[0])\n",
        "\n",
        "# turn our tokenized documents into a id <-> term dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "# convert tokenized documents into a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "#Generate LDA model\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=20)\n",
        "import pprint\n",
        "pprint.pprint(ldamodel.top_topics(corpus,topn=5))\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMWxr1wURXRb"
      },
      "source": [
        "# Standardise installed frameworks version"
      ]
    }
  ]
}